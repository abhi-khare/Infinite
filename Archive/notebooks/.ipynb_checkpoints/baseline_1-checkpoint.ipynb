{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Baseline Model  = Encoder frozen \n",
    "Dataset - mixed without up/downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer , BertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from TorchCRF import CRF\n",
    "import pandas as pd\n",
    "from torch import cuda\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter('baseline_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class jointBert(nn.Module):\n",
    "\n",
    "    def __init__(self, baseModel , num_intent , num_slots, intent_dropout=0.0,slots_dropout=0.0 ):\n",
    "\n",
    "        super(jointBert,self).__init__()\n",
    "        #self.args = args\n",
    "        self.encoder = DistilBertForSequenceClassification.from_pretrained(baseModel,num_labels=num_intent,return_dict=True,output_hidden_states=True)\n",
    "\n",
    "        #self.intent_classifier = nn.Linear(768,num_intent)\n",
    "        #self.dropout_intent = nn.Dropout(intent_dropout)\n",
    "\n",
    "        self.slot_classifier = nn.Linear(768, num_slots)\n",
    "        self.dropout_slots = nn.Dropout(slots_dropout)\n",
    "\n",
    "        self.crf = CRF(num_slots)\n",
    "\n",
    "        self.intent_loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.joint_loss_coef = 1.0\n",
    "\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, intent_target, slot_target):\n",
    "\n",
    "        encoded_output = self.encoder(input_ids, attention_mask)\n",
    "        sequence_rep = encoded_output[1][6]\n",
    "        slots_logits = self.slot_classifier(self.dropout_slots(sequence_rep))\n",
    "        \n",
    "        intent_logits = encoded_output[0]\n",
    "        \n",
    "        joint_loss = 0\n",
    "        \n",
    "        # accumulating intent classification loss \n",
    "        intent_loss = self.intent_loss(intent_logits, intent_target)\n",
    "        \n",
    "        # accumulating slot prediction loss\n",
    "        slot_loss = -1 * self.joint_loss_coef * self.crf(slots_logits, slot_target, mask=attention_mask.byte())\n",
    "        #print(slot_loss.size())\n",
    "        slot_loss = torch.mean(slot_loss)\n",
    "        joint_loss = slot_loss + intent_loss\n",
    "\n",
    "        return joint_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nlu_dataset(Dataset):\n",
    "    def __init__(self, file_dir, tokenizer, max_len):\n",
    "        \n",
    "        self.data = pd.read_csv(file_dir, sep='\\t')\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(tokenizer)\n",
    "        self.max_len = max_len\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        text = str(self.data.utterance[index])\n",
    "        text = \" \".join(text.split())\n",
    "        \n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        \n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'intent': self.data.intent[index],\n",
    "            'slot' : self.data.slot_labels[index],\n",
    "            'intent_target': torch.tensor(self.data.intent_ID[index], dtype=torch.long),\n",
    "            'slot_target' : self.data.slots_ID[index]\n",
    "        } \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-multilingual-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = jointBert(baseModel='distilbert-base-multilingual-cased',num_intent=17,num_slots=160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDS, valDS =  nlu_dataset('./data/splits/multi-train.tsv', 'distilbert-base-multilingual-cased',46), nlu_dataset('./data/splits/multi-dev.tsv','distilbert-base-multilingual-cased',46)\n",
    "trainDL = DataLoader(trainDS,batch_size=32,shuffle=True)\n",
    "valDL = DataLoader(valDS,batch_size=32,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for params in model.encoder.parameters():\n",
    "    params.requires_grad = False\n",
    "\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_label(labels, max_len):\n",
    "    slot_target = []\n",
    "        \n",
    "    for sLabel in labels:\n",
    "        slots = [int(L) for L in sLabel.split()]\n",
    "        slots += [159]*(max_len - len(slots))\n",
    "        slot_target.append(slots)\n",
    "        \n",
    "    slot_target = torch.LongTensor(slot_target)\n",
    "    return slot_target.to(device, dtype = torch.long)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-f31e059d247d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mintent_target\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mslot_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/MLENV3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/MLENV3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_or_tensors_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/MLENV3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m                 \u001b[0mnew_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# training loop\n",
    "for _ in range(1):\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    model.train()\n",
    "    # training loop\n",
    "\n",
    "    start_train = time.time()\n",
    "    for idx,batch in enumerate(trainDL,0):\n",
    "\n",
    "        ids = batch['ids'].to(device, dtype = torch.long)\n",
    "        mask = batch['mask'].to(device, dtype = torch.long)\n",
    "        intent_target = batch['intent_target'].to(device, dtype = torch.long)\n",
    "        slot_target = process_label(batch['slot_target'],46)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(ids,mask,intent_target,slot_target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #epoch_loss += loss.detach()\n",
    "        print(loss.detach())\n",
    "    \n",
    "    end_train = time.time()\n",
    "    writer.add_scalar('Loss/train', epoch_loss, _)\n",
    "    print(\"Epoch: {epoch_no} train_loss: {loss} time elapsed: {time}\".format(epoch_no = idx , loss = epoch_loss , time = end_train - start_train))\n",
    "\n",
    "    # validation loop\n",
    "    #best_eval_lo\n",
    "    if _% args.check_val_every_n_epoch == 0:\n",
    "\n",
    "        model.eval()\n",
    "        eval_loss = 0.0 \n",
    "        start_val = time.time()\n",
    "        for idx,batch in enumerate(valDL,0):\n",
    "\n",
    "            ids = batch['ids'].to(args.device, dtype = torch.long)\n",
    "            mask = batch['mask'].to(args.device, dtype = torch.long)\n",
    "            intent_target = batch['intent_target'].to(args.device, dtype = torch.long)\n",
    "            slot_target = batch['slot_target'].to(args.device, dtype = torch.long)\n",
    "\n",
    "            loss = model(ids,mask,intent_target,slot_target)\n",
    "\n",
    "            eval_loss += loss.detach()\n",
    "\n",
    "        \n",
    "        end_val = time.time()\n",
    "        writer.add_scalar('Loss/val', eval_loss, _ / args.check_val_every_n_epoch)\n",
    "        print(\"Epoch: {epoch_no} train_loss: {loss} time elapsed: {time}\".format(epoch_no = _ / args.check_val_every_n_epoch , loss = eval_loss , time = end_val - start_val))\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
