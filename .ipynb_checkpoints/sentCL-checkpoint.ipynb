{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "154c088e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import DistilBertModel\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import seed_everything, loggers as pl_loggers\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from transformers import  DistilBertModel,DistilBertTokenizer\n",
    "\n",
    "from pytorch_metric_learning import miners, losses\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from scripts.dataset import *\n",
    "from scripts.utils import *\n",
    "from arguments import jointBert_argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b20096ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameter\n",
    "config = {\n",
    "\n",
    "'mc' : {\n",
    "    'model_name' : 'distilbert-base-multilingual-cased',\n",
    "    'tokenizer_name' : 'distilbert-base-multilingual-cased',\n",
    "    'joint_loss_coef' : 0.5,\n",
    "    'id_1': 0.29868357362720055,\n",
    "    'id_2':0.2226859356474008,\n",
    "    'sd':0.3180000141987541,\n",
    "    'Ihs': 77,\n",
    "    'freeze_decoder' : True\n",
    "},\n",
    "\n",
    "# training parameters\n",
    "'tc' : {\n",
    "    'lr' : 0.00003,\n",
    "    'epoch' : 40,\n",
    "    'batch_size' : 15,\n",
    "    'weight_decay' : 0.003,\n",
    "    'shuffle_data' : True,\n",
    "    'num_worker' : 8\n",
    "},\n",
    "\n",
    "# data params\n",
    "\n",
    "'dc' : {\n",
    "    'train_dir' : '/content/drive/MyDrive/research/Infinite/data/multiATIS/split/train/WWTLE_Augmented/test_EN.tsv',\n",
    "    'val_dir' : '/content/drive/MyDrive/research/Infinite/data/multiATIS/split/valid/clean/val.tsv',\n",
    "    'intent_num' : 18,\n",
    "    'slots_num' : 159,\n",
    "    'max_len' : 56\n",
    "},\n",
    "\n",
    "# misc\n",
    "'misc' : {\n",
    "    'fix_seed' : False,\n",
    "    'gpus' : -1,\n",
    "    'log_dir' : './',\n",
    "    'precision' : 16,\n",
    "}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "08f60ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class contraNLUDataset(Dataset):\n",
    "    def __init__(self, file_dir):\n",
    "\n",
    "        self.data = pd.read_csv(file_dir, sep=\"\\t\")\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        text = str(self.data.TEXT[index])\n",
    "        text = text.replace(\".\", \"\")\n",
    "        text = text.replace(\"'\", \"\")\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        return {\n",
    "            \"text\": text,\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "class contra_Dataset_pl(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self, train_dir, val_dir, batch_size, num_worker\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.train_dir = train_dir\n",
    "        self.val_dir = val_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_worker = num_worker\n",
    "\n",
    "    def setup(self, stage: [str] = None):\n",
    "        self.train = contraNLUDataset(self.train_dir)\n",
    "\n",
    "        self.val = contraNLUDataset(self.val_dir)\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train, batch_size=self.batch_size, num_workers=self.num_worker\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val, batch_size=self.batch_size, num_workers=self.num_worker\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f1a354ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = contraNLUDataset('./data/multiATIS/split/train/clean/train.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a6891749",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(ds,batch_size=14,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6c3fd92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = DistilBertModel.from_pretrained(\n",
    "            'distilbert-base-multilingual-cased', return_dict=True, output_hidden_states=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8284c987",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-dd72bec0ce4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtoken_ids\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m163\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         inputs = tokenizer.encode_plus(b['text'][i],None,add_special_tokens=True,return_token_type_ids=False,\n\u001b[0m\u001b[1;32m      6\u001b[0m             truncation=True,max_length=56,padding=\"max_length\")\n\u001b[1;32m      7\u001b[0m         \u001b[0mtoken_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-multilingual-cased')\n",
    "for b in dl:\n",
    "    token_ids , mask = [],[]\n",
    "    for i in range(14):\n",
    "        inputs = tokenizer.encode_plus(b['text'][i],None,add_special_tokens=True,return_token_type_ids=False,\n",
    "            truncation=True,max_length=56,padding=\"max_length\")\n",
    "        token_ids.append(inputs[\"input_ids\"])\n",
    "        mask.append(inputs[\"attention_mask\"])\n",
    "    \n",
    "    token_ids = torch.tensor(token_ids, dtype=torch.long)\n",
    "    mask = torch.tensor(mask, dtype=torch.long)\n",
    "    hidden = encoder(token_ids,mask)\n",
    "    \n",
    "    print(token_ids,mask,hidden[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9eae1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
