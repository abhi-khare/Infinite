{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619855c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from functools import partial\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0566e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/BG_Noise_Phrase.txt\") as f:\n",
    "    content = f.readlines()\n",
    "phrase = [x.strip() for x in content]\n",
    "\n",
    "# contrastive noise augmentation samples\n",
    "\n",
    "\n",
    "def mergelistsBG(ns, s, prob):\n",
    "\n",
    "    noisySample = deepcopy(ns)\n",
    "    sample = deepcopy(s)\n",
    "\n",
    "    bernaulliSample = [0] * int((1000) * prob) + [1] * int(1000 * (1 - prob))\n",
    "    random.shuffle(bernaulliSample)\n",
    "\n",
    "    final = []\n",
    "\n",
    "    while len(noisySample) > 0 and len(sample) > 0:\n",
    "\n",
    "        if random.sample(bernaulliSample, 1)[0] == 0:\n",
    "            final.append(noisySample.pop(0))\n",
    "        else:\n",
    "            final.append(sample.pop(0))\n",
    "\n",
    "    if len(noisySample) == 0:\n",
    "        final = final + sample\n",
    "    else:\n",
    "        final = final + noisySample\n",
    "\n",
    "    return s,final\n",
    "\n",
    "def mergelistsMC(text_packed, prob):\n",
    "\n",
    "    text = deepcopy(text_packed)\n",
    "\n",
    "    bernaulliSample = [0] * int((1000) * prob) + [1] * int(1000 * (1 - prob))\n",
    "    random.shuffle(bernaulliSample)\n",
    "\n",
    "    orig,aug  = [text_packed[0]],[text_packed[0]]\n",
    "    for idx,tokens in enumerate(text_packed[1:]):\n",
    "        \n",
    "        if random.sample(bernaulliSample, 1)[0] == 0:\n",
    "            orig.append([tokens[0],'2000'])\n",
    "        else:\n",
    "            orig.append(tokens)\n",
    "            aug.append(tokens)\n",
    "\n",
    "    return orig,aug\n",
    "\n",
    "def contrastiveSampleGenerator(sample, noise_type):\n",
    "\n",
    "    samplePacked = [[token, str(idx)] for idx, token in enumerate(sample.split())]\n",
    "\n",
    "    noisyTEXT = random.sample(phrase, 3)\n",
    "    noisyTEXT = (noisyTEXT[0] + noisyTEXT[1] + noisyTEXT[2]).split()\n",
    "    noisyTOKENS = random.sample(noisyTEXT, random.sample([5, 6, 7,8,9,10], 1)[0])\n",
    "    noisyPacked = [[token, '2000'] for idx, token in enumerate(noisyTOKENS)]\n",
    "\n",
    "    if noise_type == 'MC':\n",
    "        noise_param = random.sample([0.20,0.40,0.60],1)[0]\n",
    "        orig, aug = mergelistsMC(samplePacked, prob=noise_param)\n",
    "        augText, augSlots = zip(*aug)\n",
    "        origText, origSlots = zip(*orig)\n",
    "\n",
    "        return ' '.join(list(origText)), ' '.join(list(augText)), ' '.join(list(origSlots)), ' '.join(list(augSlots))\n",
    "\n",
    "    elif noise_type == 'BG':\n",
    "        noise_param = random.sample([0.20,0.40,0.60],1)[0]\n",
    "        orig, aug  = mergelistsBG(noisyPacked,samplePacked,  prob=noise_param)\n",
    "        augText, augSlots = zip(*aug)\n",
    "        origText, origSlots = zip(*orig)\n",
    "        return ' '.join(list(origText)), ' '.join(list(augText)), ' '.join(list(origSlots)), ' '.join(list(augSlots))\n",
    "\n",
    "def contrastivePairs(text, noise_type):\n",
    "\n",
    "    textP1, textP2, slotsID1, slotsID2, sentID1, sentID2 = [], [], [], [], [], []\n",
    "\n",
    "    for idx, sample in enumerate(text):\n",
    "\n",
    "        origText,augText, origSlots, augSlots = contrastiveSampleGenerator(sample,noise_type)\n",
    "          \n",
    "        textP1.append(origText)\n",
    "        slotsID1.append(origSlots)\n",
    "        sentID1.append(idx)\n",
    "\n",
    "        textP2.append(augText)\n",
    "        slotsID2.append(augSlots)\n",
    "        sentID2.append(idx)\n",
    "\n",
    "    return textP1, textP2, slotsID1, slotsID2, sentID1, sentID2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b16fd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_CT_AT(batch, tokenizer, noise_type):\n",
    "\n",
    "    text,intent_id,slot_id = [],[],[]\n",
    "    \n",
    "    for datapoint in batch:\n",
    "        text.append(datapoint['text'])\n",
    "        intent_id.append(datapoint['intent_id'])\n",
    "        slot_id.append(datapoint['slots_id'])\n",
    "\n",
    "    # processing batch for hierarchial contrastive learning\n",
    "\n",
    "    # generating contrastive pairs\n",
    "    textP1, textP2, tokenID1, tokenID2, sentID1, sentID2 = contrastivePairs(\n",
    "        text,noise_type\n",
    "    )\n",
    "\n",
    "    # tokenization and packing for pair 1\n",
    "    token_ids1, mask1, processed_tokenID1 = batch_tokenizer(textP1, tokenID1, tokenizer)\n",
    "    token_ids1, mask1, sentID1, packed_tokenID1 = list2Tensor(\n",
    "        [token_ids1, mask1, sentID1, processed_tokenID1]\n",
    "    )\n",
    "\n",
    "    # tokenization and packing for pair 2\n",
    "    token_ids2, mask2, processed_tokenID2 = batch_tokenizer(textP2, tokenID2, tokenizer)\n",
    "    token_ids2, mask2, sentID2, packed_tokenID2 = list2Tensor(\n",
    "        [token_ids2, mask2, sentID2, processed_tokenID2]\n",
    "    )\n",
    "\n",
    "    CP1 = {\n",
    "        \"token_ids\": token_ids1,\n",
    "        \"mask\": mask1,\n",
    "        \"sent_id\": sentID1,\n",
    "        \"token_id\": packed_tokenID1,\n",
    "    }\n",
    "\n",
    "    CP2 = {\n",
    "        \"token_ids\": token_ids2,\n",
    "        \"mask\": mask2,\n",
    "        \"sent_id\": sentID2,\n",
    "        \"token_id\": packed_tokenID2,\n",
    "    }\n",
    "\n",
    "    # processing batch for adversarial examples\n",
    "    adv_text,adv_intent = textP2, intent_id\n",
    "    adv_slots = []\n",
    "    for slots in token\n",
    "\n",
    "    return {\"supBatch\": supBatch, \"HCLBatch\": [CP1, CP2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d29e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "    def __init__(self, file_dir):\n",
    "\n",
    "        self.data = pd.read_csv(file_dir, sep=\"\\t\")\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        # text\n",
    "        text = str(self.data.TEXT[index])\n",
    "        \n",
    "        # intent\n",
    "        intent_label = self.data.INTENT[index]\n",
    "        intent_id = self.data.INTENT_ID[index]\n",
    "        \n",
    "        # slots\n",
    "        slot_label = self.data.SLOTS[index]\n",
    "        slot_id = self.data.SLOTS_ID[index]\n",
    "\n",
    "        return {\n",
    "\n",
    "            \"text\": text,\n",
    "\n",
    "            \"intent_id\": intent_id,\n",
    "            \"intent_label\": intent_label,\n",
    "\n",
    "            \"slots_id\": slot_id,\n",
    "            \"slots_label\": slot_label,\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e6bac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataloader(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__( self,train_dir,batch_size,num_workers):\n",
    "\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.num_worker = num_workers\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('distilbert-base-cased',cache_dir = '/efs-storage/tokenizer/')\n",
    "        self.mode = args.mode\n",
    "        self.args = args\n",
    "\n",
    "    def setup(self, stage: [str] = None):\n",
    "\n",
    "        self.train = dataset(self.train_dir)\n",
    "\n",
    "        self.val = dataset(self.val_dir)\n",
    "\n",
    "        if self.mode == 'BASELINE':\n",
    "            self.train_collate = partial(collate_sup,tokenizer = self.tokenizer)\n",
    "            self.val_collate = partial(collate_sup, tokenizer = self.tokenizer)\n",
    "        \n",
    "        elif self.mode == 'AT':\n",
    "            self.train_collate = partial(collate_AT,tokenizer = self.tokenizer, noise_type = self.args.noise_type)\n",
    "            self.val_collate = partial(collate_sup,tokenizer = self.tokenizer)\n",
    "        \n",
    "        elif self.mode == 'CT':\n",
    "            self.train_collate = partial(collate_CT,tokenizer = self.tokenizer, noise_type = self.args.noise_type)\n",
    "            self.val_collate = partial(collate_sup,tokenizer = self.tokenizer)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train, batch_size=self.batch_size, shuffle=True, collate_fn=self.train_collate, num_workers=self.num_worker\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val, batch_size=self.batch_size, collate_fn=self.val_collate, num_workers=self.num_worker\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8170bcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class hierCon_model(nn.Module):\n",
    "\n",
    "    def __init__(self, args):\n",
    "\n",
    "        super(hierCon_model, self).__init__()\n",
    "\n",
    "        self.encoder = DistilBertModel.from_pretrained(\n",
    "            args.encoder, return_dict=True, output_hidden_states=True,\n",
    "            sinusoidal_pos_embds=True, cache_dir='/efs-storage/model/'\n",
    "        )\n",
    "        \n",
    "        self.intent_head = nn.Sequential(\n",
    "                                         nn.Dropout(args.intent_dropout),\n",
    "                                         nn.Linear(256,args.intent_count)\n",
    "                                        )\n",
    "\n",
    "        self.slots_head = nn.Sequential(\n",
    "                                         nn.Dropout(args.slots_dropout),\n",
    "                                         nn.Linear(256,args.slots_count)\n",
    "                                        )\n",
    "\n",
    "        self.token_contrast_proj = nn.Sequential(\n",
    "                                                 nn.Linear(768,512),\n",
    "                                                 nn.BatchNorm1d(512),\n",
    "                                                 nn.ReLU(inplace=True),\n",
    "                                                 nn.Linear(512,256),\n",
    "                                                 nn.ReLU(inplace=True)\n",
    "                                                )\n",
    "        \n",
    "        self.sent_contrast_proj = nn.Sequential(\n",
    "                                                 nn.Linear(768,512),\n",
    "                                                 nn.BatchNorm1d(512),\n",
    "                                                 nn.ReLU(inplace=True),\n",
    "                                                 nn.Linear(512,256),\n",
    "                                                 nn.ReLU(inplace=True) \n",
    "                                                 )\n",
    "        self.intent_predictor = nn.Sequential(\n",
    "                                         nn.Linear(256,256),\n",
    "                                        nn.BatchNorm1d(256),\n",
    "                                        nn.ReLU(inplace=True),\n",
    "                                        nn.Linear(256,256)\n",
    "                                        )\n",
    "\n",
    "        self.slots_predictor = nn.Sequential(\n",
    "                                         nn.Linear(256,256),\n",
    "                                        nn.BatchNorm1d(256),\n",
    "                                        nn.ReLU(inplace=True),\n",
    "                                        nn.Linear(256,256)\n",
    "                                        )\n",
    "\n",
    "        \n",
    "        self.criterion = nn.CosineSimilarity(dim=1)\n",
    "        \n",
    "        self.CE_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.icnerCoef = args.icnerCoef\n",
    "        self.hierConCoef = args.hierConCoef\n",
    "        self.args = args\n",
    "\n",
    "    def ICNER_loss(self, encoded_output, intent_target, slots_target):\n",
    "\n",
    "        # intent prediction loss\n",
    "        intent_hidden = encoded_output[0][:, 0]\n",
    "        intent_logits = self.intent_head(self.sent_contrast_proj(intent_hidden))\n",
    "        intent_loss = self.CE_loss(intent_logits, intent_target)\n",
    "        intent_pred = torch.argmax(nn.Softmax(dim=1)(intent_logits), axis=1)\n",
    "\n",
    "        # slots prediction loss\n",
    "\n",
    "        shape = encoded_output[0].shape\n",
    "        slots_hidden = encoded_output[0].view(shape[0]*shape[1],-1)\n",
    "        slots_logits = self.slots_head(self.token_contrast_proj(slots_hidden))\n",
    "        slots_pred = torch.argmax(nn.Softmax(dim=1)(slots_logits), axis=1)\n",
    "        slots_pred = slots_pred.view(shape[0],-1)\n",
    "        slots_loss = self.CE_loss(\n",
    "            slots_logits.view(-1, self.args.slots_count), slots_target.view(-1)\n",
    "        )\n",
    "\n",
    "        joint_loss = self.icnerCoef * intent_loss + (1.0 - self.icnerCoef) * slots_loss\n",
    "\n",
    "        return {\n",
    "            \"joint_loss\": joint_loss,\n",
    "            \"ic_loss\": intent_loss,\n",
    "            \"ner_loss\": slots_loss,\n",
    "            \"intent_pred\": intent_pred,\n",
    "            \"slot_pred\": slots_pred,\n",
    "        }\n",
    "\n",
    "    def sentCL(self, sentz1, sentz2):\n",
    "\n",
    "        # calculating sentence level loss\n",
    "        p1, p2 = self.sent_contrast_proj(sentz1), self.sent_contrast_proj(sentz2)\n",
    "        z1, z2 = self.intent_predictor(p1) , self.intent_predictor(p2) \n",
    "        p1.detach()\n",
    "        p2.detach()\n",
    "        \n",
    "        sentCLLoss =  -(self.criterion(p2, z1).mean() + self.criterion(p1, z2).mean()) * 0.5\n",
    "\n",
    "        return sentCLLoss\n",
    "    \n",
    "    def tokenCL(self, tokenEmb1,tokenEmb2,tokenID1,tokenID2):\n",
    "        #torch.Size([32, 56, 768]) torch.Size([32, 56, 768]) torch.Size([1792]) torch.Size([1792])\n",
    "        tokenID1 = torch.flatten(tokenID1)\n",
    "        tokenID2 = torch.flatten(tokenID2)\n",
    "        \n",
    "        shape = tokenEmb1.shape\n",
    "        tokenEmb1 = tokenEmb1.view(shape[0]*shape[1],-1)\n",
    "        tokenEmb2 = tokenEmb2.view(shape[0]*shape[1],-1) #torch.Size([1792, 768]) torch.Size([1792, 768])\n",
    "        \n",
    "        filterTokenIdx1 = [idx for idx,val in enumerate(tokenID1.tolist()) if (val==-100 or val == 2000)!=True]\n",
    "        filterTokenIdx2 = [idx for idx,val in enumerate(tokenID2.tolist()) if (val==-100 or val == 2000)!=True]\n",
    "\n",
    "        if len(filterTokenIdx1) > 0:\n",
    "            filterTokenIdx1 = torch.tensor( filterTokenIdx1,dtype=torch.long,device=torch.device('cuda'))\n",
    "            tokenEmb1 = torch.index_select(tokenEmb1,0,filterTokenIdx1) \n",
    "        \n",
    "        if len(filterTokenIdx2) > 0:\n",
    "            filterTokenIdx2 = torch.tensor( filterTokenIdx2,dtype=torch.long,device=torch.device('cuda')) \n",
    "            tokenEmb2 = torch.index_select(tokenEmb2,0,filterTokenIdx2)\n",
    "        \n",
    "        # calculating sentence level loss\n",
    "        p1, p2 = self.token_contrast_proj(tokenEmb1), self.token_contrast_proj(tokenEmb2)\n",
    "        z1, z2 = self.intent_predictor(p1) , self.intent_predictor(p2) \n",
    "        p1.detach()\n",
    "        p2.detach()\n",
    "        tokenCLLoss =  -(self.criterion(p2, z1).mean() + self.criterion(p1, z2).mean()) * 0.5\n",
    "            \n",
    "        return tokenCLLoss\n",
    "\n",
    "    def forward(self, batch , mode):\n",
    "\n",
    "        if mode == \"ICNER\":\n",
    "            encoded_output = self.encoder(batch['supBatch']['token_ids'], batch['supBatch']['mask'])\n",
    "            return self.ICNER_loss(encoded_output, batch['supBatch']['intent_id'], batch['supBatch']['slots_id'])\n",
    "\n",
    "        if mode == \"hierCon\":\n",
    "            encoded_output_0 = self.encoder(batch['HCLBatch'][0]['token_ids'],batch['HCLBatch'][0]['mask']) \n",
    "            encoded_output_1 = self.encoder(batch['HCLBatch'][1]['token_ids'],batch['HCLBatch'][1]['mask']) \n",
    "            sentCL = self.sentCL(encoded_output_0[0][:, 0], encoded_output_1[0][:, 0])\n",
    "            \n",
    "            tokenIDs0 = batch['HCLBatch'][0]['token_id']\n",
    "            tokenIDs1 = batch['HCLBatch'][1]['token_id']\n",
    "            \n",
    "            tokenCL = self.tokenCL(encoded_output_0[0], encoded_output_1[0],tokenIDs0,tokenIDs1)\n",
    "\n",
    "            hierConLoss = self.args.hierConCoef*sentCL + (1.0-self.args.hierConCoef)*tokenCL\n",
    "\n",
    "            return hierConLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277524c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
