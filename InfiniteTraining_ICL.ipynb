{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021db68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import sys\n",
    "sys.path.append('/content/drive/MyDrive/research/Infinite/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9866c32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers\n",
    "! pip install seqeval\n",
    "! pip install pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e43bcbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import DistilBertModel\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import seed_everything, loggers as pl_loggers\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from transformers import  DistilBertModel,DistilBertTokenizerFast\n",
    "\n",
    "from pytorch_metric_learning import miners, losses\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from scripts.dataset import *\n",
    "from scripts.utils import *\n",
    "from arguments import jointBert_argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b0b47ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameter\n",
    "config = {\n",
    "\n",
    "'mc' : {\n",
    "    'model_name' : 'distilbert-base-multilingual-cased',\n",
    "    'tokenizer_name' : 'distilbert-base-multilingual-cased',\n",
    "    'joint_loss_coef' : 0.5,\n",
    "    'id_1': 0.29868357362720055,\n",
    "    'id_2':0.2226859356474008,\n",
    "    'sd':0.3180000141987541,\n",
    "    'Ihs': 77,\n",
    "    'freeze_decoder' : True\n",
    "},\n",
    "\n",
    "# training parameters\n",
    "'tc' : {\n",
    "    'lr' : 0.00003,\n",
    "    'epoch' : 40,\n",
    "    'batch_size' : 15,\n",
    "    'weight_decay' : 0.003,\n",
    "    'shuffle_data' : True,\n",
    "    'num_worker' : 8\n",
    "},\n",
    "\n",
    "# data params\n",
    "\n",
    "'dc' : {\n",
    "    'train_dir' : '/content/drive/MyDrive/research/Infinite/data/multiATIS/split/train/WWTLE_Augmented/test_EN.tsv',\n",
    "    'val_dir' : '/content/drive/MyDrive/research/Infinite/data/multiATIS/split/valid/clean/val.tsv',\n",
    "    'intent_num' : 18,\n",
    "    'slots_num' : 159,\n",
    "    'max_len' : 56\n",
    "},\n",
    "\n",
    "# misc\n",
    "'misc' : {\n",
    "    'fix_seed' : False,\n",
    "    'gpus' : -1,\n",
    "    'log_dir' : './',\n",
    "    'precision' : 16,\n",
    "}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36208ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading slot index file\n",
    "final_slots = pd.read_csv('/content/drive/MyDrive/research/Infinite/data/multiATIS/slots_list.csv',sep=',',header=None,names=['SLOTS']).SLOTS.values.tolist()\n",
    "idx2slots  = {idx:slots for idx,slots in enumerate(final_slots)}\n",
    "\n",
    "# callback for pytorch lightning\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath='/content/drive/MyDrive/research/Infinite/bin/ICL/',\n",
    "    filename='Infinite-{epoch:02d}-{val_loss:.2f}',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "694393e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Infinite_encoder(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "\n",
    "        super(Infinite_encoder, self).__init__()\n",
    "\n",
    "        self.encoder = DistilBertModel.from_pretrained(\n",
    "            cfg[\"mc\"][\"model_name\"], return_dict=True, output_hidden_states=True\n",
    "        )\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        self.intent_loss = losses.NTXentLoss()\n",
    "        self.slot_loss = losses.NTXentLoss()\n",
    "        \n",
    "\n",
    "    def forward(self, input_ids, attention_mask, intent_target, slots_target):\n",
    "\n",
    "        encoded_output = self.encoder(input_ids, attention_mask)\n",
    "\n",
    "        # intent data flow\n",
    "        intent_hidden = encoded_output[0][:, 0]\n",
    "     \n",
    "        # accumulating intent contrastive loss\n",
    "        intent_loss = self.intent_loss(intent_hidden,intent_target)\n",
    "        \n",
    "        return intent_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12007e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "contraDL = contra_pl('/content/drive/MyDrive/research/Infinite/data/multiATIS/split/train/contraSet/train_EN.tsv',\n",
    "               config['mc']['tokenizer_name'], config['dc']['max_len'],\n",
    "               config['tc']['batch_size'],\n",
    "                    config['tc']['num_worker'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1126d589",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Infinite_ICL_training(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.Infinite_encoder = Infinite_encoder(cfg)\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def forward(self, input_ids, attention_mask , intent_target, slots_target):\n",
    "        return self.Infinite_encoder(input_ids, attention_mask , intent_target, slots_target)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        token_ids, attention_mask = batch['token_ids'], batch['mask']\n",
    "        intent_target,slots_target = batch['sent_id'], batch['slot_id']\n",
    "        \n",
    "        out = self(token_ids,attention_mask,intent_target,slots_target)\n",
    "        \n",
    "        self.log('train_ICL', out, on_step=False, on_epoch=True, logger=True)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "         return torch.optim.AdamW( self.parameters(), lr=config['tc']['lr'] ,  weight_decay=self.cfg['tc']['weight_decay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8dd8602d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Infinite_ICL_training(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657259fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Using native 16bit precision.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type             | Params\n",
      "------------------------------------------------------\n",
      "0 | Infinite_encoder | Infinite_encoder | 134 M \n",
      "------------------------------------------------------\n",
      "134 M     Trainable params\n",
      "0         Non-trainable params\n",
      "134 M     Total params\n",
      "538.936   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e22155743aa64fe0a7fa4920b0b59d1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model training\n",
    "trainer = pl.Trainer(gpus=config['misc']['gpus'],callbacks=[checkpoint_callback] ,accumulate_grad_batches=4,precision=config['misc']['precision'],max_epochs=config['tc']['epoch'])\n",
    "\n",
    "trainer.fit(model, contraDL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc1b648",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
