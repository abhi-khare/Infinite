{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeee68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import sys\n",
    "sys.path.append('/content/drive/MyDrive/research/Infinite/')\n",
    "\n",
    "! pip install optuna\n",
    "! pip install pytorch-lightning\n",
    "! pip install transformers\n",
    "! pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e140ffd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import DistilBertModel, DistilBertTokenizerFast\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from scripts.dataset import *\n",
    "from scripts.utils import *\n",
    "\n",
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "077a9145",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f4de9f3176d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# loading slot index file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m final_slots = pd.read_csv(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m\"/content/drive/MyDrive/research/Infinite/data/ATIS/slots_list.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"SLOTS\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m ).SLOTS.values.tolist()\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# loading slot index file\n",
    "final_slots = pd.read_csv(\n",
    "    \"/content/drive/MyDrive/research/Infinite/data/ATIS/slots_list.csv\", sep=\",\", header=None, names=[\"SLOTS\"]\n",
    ").SLOTS.values.tolist()\n",
    "\n",
    "idx2slots = {idx: slots for idx, slots in enumerate(final_slots)}\n",
    "\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4d2b490a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameter\n",
    "config = {\n",
    "    \"mc\": {\n",
    "        \"model_name\": \"distilbert-base-multilingual-cased\",\n",
    "        \"tokenizer_name\": \"distilbert-base-multilingual-cased\",\n",
    "        \"joint_loss_coef\": 0.5,\n",
    "    },\n",
    "    # training parameters\n",
    "    \"tc\": {\n",
    "        \"lr\": 0.00003,\n",
    "        \"epoch\": 20,\n",
    "        \"batch_size\": 64,\n",
    "        \"weight_decay\": 0.003,\n",
    "        \"shuffle_data\": True,\n",
    "        \"num_worker\": 2,\n",
    "    },\n",
    "    # data params\n",
    "    \"dc\": {\n",
    "        \"train_dir\": \"/content/drive/MyDrive/research/Infinite/data/ATIS/experiment/train/clean/train.tsv\",\n",
    "        \"val_dir\": \"/content/drive/MyDrive/research/Infinite/data/ATIS/experiment/dev/clean/dev.tsv\",\n",
    "        \"max_len\": 56,\n",
    "    },\n",
    "    # misc\n",
    "    \"misc\": {\n",
    "        \"fix_seed\": False,\n",
    "        \"gpus\": -1,\n",
    "        \"precision\": 16,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a19966fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IC_NER(nn.Module):\n",
    "    def __init__(self, idropout_1, idropout_2, sdropout, ihidden_size):\n",
    "\n",
    "        super(IC_NER, self).__init__()\n",
    "\n",
    "        self.encoder = DistilBertModel.from_pretrained(\n",
    "            \"distilbert-base-multilingual-cased\",\n",
    "            return_dict=True,\n",
    "            output_hidden_states=True,\n",
    "            sinusoidal_pos_embds=True\n",
    "        )\n",
    "\n",
    "        self.intent_dropout_1 = nn.Dropout(idropout_1)\n",
    "        self.intent_dropout_2 = nn.Dropout(idropout_2)\n",
    "        self.intent_FC1 = nn.Linear(768, ihidden_size)\n",
    "        self.intent_FC2 = nn.Linear(ihidden_size, 18)\n",
    "\n",
    "        # slots layer\n",
    "        self.slots_dropout = nn.Dropout(sdropout)\n",
    "        self.slots_FC = nn.Linear(768, 120)\n",
    "\n",
    "        self.intent_loss_fn = nn.CrossEntropyLoss()\n",
    "        self.slot_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.jlc = 0.5\n",
    "        # self.cfg = cfg\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, intent_target, slots_target):\n",
    "\n",
    "        encoded_output = self.encoder(input_ids, attention_mask)\n",
    "\n",
    "        # intent data flow\n",
    "        intent_hidden = encoded_output[0][:, 0]\n",
    "        intent_hidden = self.intent_FC1(self.intent_dropout_1(F.gelu(intent_hidden)))\n",
    "        intent_logits = self.intent_FC2(self.intent_dropout_2(F.gelu(intent_hidden)))\n",
    "\n",
    "        # accumulating intent classification loss\n",
    "        intent_loss = self.intent_loss_fn(intent_logits, intent_target)\n",
    "        intent_pred = torch.argmax(nn.Softmax(dim=1)(intent_logits), axis=1)\n",
    "\n",
    "        # slots data flow\n",
    "        slots_hidden = encoded_output[0]\n",
    "        slots_logits = self.slots_FC(self.slots_dropout(F.relu(slots_hidden)))\n",
    "        slot_pred = torch.argmax(nn.Softmax(dim=2)(slots_logits), axis=2)\n",
    "\n",
    "        # accumulating slot prediction loss\n",
    "        slot_loss = self.slot_loss_fn(slots_logits.view(-1, 120), slots_target.view(-1))\n",
    "\n",
    "        joint_loss = self.jlc * intent_loss + (1.0 - self.jlc) * slot_loss\n",
    "\n",
    "        return {\n",
    "            \"joint_loss\": joint_loss,\n",
    "            \"ic_loss\": intent_loss,\n",
    "            \"ner_loss\": slot_loss,\n",
    "            \"intent_pred\": intent_pred,\n",
    "            \"slot_pred\": slot_pred,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "79e85668",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "\n",
    "    # We optimize the number of layers, hidden units in each layer and dropouts.\n",
    "    ihidden_size = trial.suggest_int(\"intent_hidden_size\", 64, 512)\n",
    "    idropout_1 = trial.suggest_float(\"idropout1\", 0.2, 0.5)\n",
    "    idropout_2 = trial.suggest_float(\"idropout2\", 0.2, 0.5)\n",
    "    sdropout = trial.suggest_float(\"sdropout\", 0.2, 0.5)\n",
    "    \n",
    "\n",
    "    model = IC_NER(idropout_1, idropout_2, sdropout, ihidden_size).to(DEVICE)\n",
    "\n",
    "    dm = NLU_Dataset_pl(\n",
    "        config[\"dc\"][\"train_dir\"],\n",
    "        config[\"dc\"][\"val_dir\"],\n",
    "        config[\"dc\"][\"val_dir\"],\n",
    "        config[\"mc\"][\"tokenizer_name\"],\n",
    "        config[\"dc\"][\"max_len\"],\n",
    "        config[\"tc\"][\"batch_size\"],\n",
    "        config[\"tc\"][\"num_worker\"],\n",
    "    )\n",
    "    dm.setup()\n",
    "    \n",
    "    trainDL, valDL = dm.train_dataloader() , dm.val_dataloader()\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "            model.parameters(), lr=config[\"tc\"][\"lr\"], weight_decay=config[\"tc\"][\"weight_decay\"]\n",
    "        )\n",
    "\n",
    "    # training\n",
    "    model.train()\n",
    "    for epoch in range(config['tc']['epoch']):\n",
    "        \n",
    "        for batch in trainDL:\n",
    "            token_ids, attention_mask = batch[\"token_ids\"].to(DEVICE), batch[\"mask\"].to(DEVICE)\n",
    "            intent_target, slots_target = batch[\"intent_id\"].to(DEVICE), batch[\"slots_id\"].to(DEVICE)\n",
    "\n",
    "            out = model(token_ids, attention_mask, intent_target, slots_target)\n",
    "            optimizer.zero_grad()\n",
    "            out[\"joint_loss\"].backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    #validation\n",
    "\n",
    "    acc,slotsF1,cnt = 0.0,0.0,0\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for batch in valDL:\n",
    "\n",
    "            token_ids, attention_mask = batch[\"token_ids\"].to(DEVICE), batch[\"mask\"].to(DEVICE)\n",
    "            intent_target, slots_target = batch[\"intent_id\"].to(DEVICE), batch[\"slots_id\"].to(DEVICE)\n",
    "\n",
    "            out = model(token_ids, attention_mask, intent_target, slots_target)\n",
    "            intent_pred, slot_pred = out[\"intent_pred\"], out[\"slot_pred\"]\n",
    "            \n",
    "            acc += accuracy(out[\"intent_pred\"], intent_target)\n",
    "            slotsF1 += slot_F1(out[\"slot_pred\"], slots_target, idx2slots)\n",
    "            cnt += 1\n",
    "        \n",
    "    acc = acc/float(cnt)\n",
    "    slotsF1 = slotsF1/float(cnt)\n",
    "\n",
    "    return acc, slotsF1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0cecba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-04-24 16:30:54,109]\u001b[0m A new study created in memory with name: no-name-03497d8a-5928-4b14-ac4b-a62987c72375\u001b[0m\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Using native 16bit precision.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name   | Type   | Params\n",
      "----------------------------------\n",
      "0 | IC_NER | IC_NER | 134 M \n",
      "----------------------------------\n",
      "134 M     Trainable params\n",
      "0         Non-trainable params\n",
      "134 M     Total params\n",
      "539.583   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2ed5121c9974a908326f1872852a1b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "sampler = optuna.multi_objective.samplers.MOTPEMultiObjectiveSampler(n_startup_trials=21,\n",
    "                                                                     n_ehvi_candidates=24)\n",
    "study = optuna.multi_objective.create_study(directions=[\"maximize\",\"maximize\"])\n",
    "\n",
    "\n",
    "study.optimize(objective, n_trials=50, timeout=100000)\n",
    "\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5baf7dfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
