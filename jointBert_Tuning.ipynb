{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeee68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e140ffd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import seed_everything, loggers as pl_loggers\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from transformers import DistilBertModel, DistilBertTokenizerFast\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from scripts.dataset import *\n",
    "from scripts.utils import *\n",
    "\n",
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "077a9145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading slot index file\n",
    "final_slots = pd.read_csv(\n",
    "    \"./data/multiATIS/slots_list.csv\", sep=\",\", header=None, names=[\"SLOTS\"]\n",
    ").SLOTS.values.tolist()\n",
    "\n",
    "idx2slots = {idx: slots for idx, slots in enumerate(final_slots)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4d2b490a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameter\n",
    "config = {\n",
    "    \"mc\": {\n",
    "        \"model_name\": \"distilbert-base-multilingual-cased\",\n",
    "        \"tokenizer_name\": \"distilbert-base-multilingual-cased\",\n",
    "        \"joint_loss_coef\": 0.5,\n",
    "    },\n",
    "    # training parameters\n",
    "    \"tc\": {\n",
    "        \"lr\": 0.00005,\n",
    "        \"epoch\": 13,\n",
    "        \"batch_size\": 64,\n",
    "        \"weight_decay\": 0.003,\n",
    "        \"shuffle_data\": True,\n",
    "        \"num_worker\": 8,\n",
    "    },\n",
    "    # data params\n",
    "    \"dc\": {\n",
    "        \"train_dir\": \"./data/multiATIS/split/train/clean/train.tsv\",\n",
    "        \"val_dir\": \"./data/multiATIS/split/valid/clean/val.tsv\",\n",
    "        \"max_len\": 56,\n",
    "    },\n",
    "    # misc\n",
    "    \"misc\": {\n",
    "        \"fix_seed\": False,\n",
    "        \"gpus\": -1,\n",
    "        \"log_dir\": \"./\",\n",
    "        \"precision\": 16,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a19966fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IC_NER(nn.Module):\n",
    "    def __init__(self, idropout_1, idropout_2, sdropout, ihidden_size):\n",
    "\n",
    "        super(IC_NER, self).__init__()\n",
    "\n",
    "        self.encoder = DistilBertModel.from_pretrained(\n",
    "            \"distilbert-base-multilingual-cased\",\n",
    "            return_dict=True,\n",
    "            output_hidden_states=True,\n",
    "        )\n",
    "\n",
    "        self.intent_dropout_1 = nn.Dropout(idropout_1)\n",
    "        self.intent_dropout_2 = nn.Dropout(idropout_2)\n",
    "        self.intent_FC1 = nn.Linear(768, ihidden_size)\n",
    "        self.intent_FC2 = nn.Linear(ihidden_size, 18)\n",
    "\n",
    "        # slots layer\n",
    "        self.slots_dropout = nn.Dropout(sdropout)\n",
    "        self.slots_FC = nn.Linear(768, 159)\n",
    "\n",
    "        self.intent_loss_fn = nn.CrossEntropyLoss()\n",
    "        self.slot_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.jlc = 0.5\n",
    "        # self.cfg = cfg\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, intent_target, slots_target):\n",
    "\n",
    "        encoded_output = self.encoder(input_ids, attention_mask)\n",
    "\n",
    "        # intent data flow\n",
    "        intent_hidden = encoded_output[0][:, 0]\n",
    "        intent_hidden = self.intent_FC1(self.intent_dropout_1(F.gelu(intent_hidden)))\n",
    "        intent_logits = self.intent_FC2(self.intent_dropout_2(F.gelu(intent_hidden)))\n",
    "\n",
    "        # accumulating intent classification loss\n",
    "        intent_loss = self.intent_loss_fn(intent_logits, intent_target)\n",
    "        intent_pred = torch.argmax(nn.Softmax(dim=1)(intent_logits), axis=1)\n",
    "\n",
    "        # slots data flow\n",
    "        slots_hidden = encoded_output[0]\n",
    "        slots_logits = self.slots_FC(self.slots_dropout(F.relu(slots_hidden)))\n",
    "        slot_pred = torch.argmax(nn.Softmax(dim=2)(slots_logits), axis=2)\n",
    "\n",
    "        # accumulating slot prediction loss\n",
    "        slot_loss = self.slot_loss_fn(slots_logits.view(-1, 159), slots_target.view(-1))\n",
    "\n",
    "        joint_loss = self.jlc * intent_loss + (1.0 - self.jlc) * slot_loss\n",
    "\n",
    "        return {\n",
    "            \"joint_loss\": joint_loss,\n",
    "            \"ic_loss\": intent_loss,\n",
    "            \"ner_loss\": slot_loss,\n",
    "            \"intent_pred\": intent_pred,\n",
    "            \"slot_pred\": slot_pred,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c2ac633f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class jointBert(pl.LightningModule):\n",
    "    def __init__(self, cfg, idropout_1, idropout_2, sdropout, ihidden_size, lr):\n",
    "        super().__init__()\n",
    "        self.IC_NER = IC_NER(idropout_1, idropout_2, sdropout, ihidden_size)\n",
    "        self.cfg = cfg\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, intent_target, slots_target):\n",
    "        return self.IC_NER(input_ids, attention_mask, intent_target, slots_target)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        token_ids, attention_mask = batch[\"token_ids\"], batch[\"mask\"]\n",
    "        intent_target, slots_target = batch[\"intent_id\"], batch[\"slots_id\"]\n",
    "\n",
    "        out = self(token_ids, attention_mask, intent_target, slots_target)\n",
    "\n",
    "        self.log(\n",
    "            \"train_IC_NER_loss\",\n",
    "            out[\"joint_loss\"],\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"train_IC_loss\", out[\"ic_loss\"], on_step=False, on_epoch=True, logger=True\n",
    "        )\n",
    "        self.log(\n",
    "            \"train_NER_loss\", out[\"ner_loss\"], on_step=False, on_epoch=True, logger=True\n",
    "        )\n",
    "\n",
    "        return out[\"joint_loss\"]\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "\n",
    "        token_ids, attention_mask = batch[\"token_ids\"], batch[\"mask\"]\n",
    "        intent_target, slots_target = batch[\"intent_id\"], batch[\"slots_id\"]\n",
    "\n",
    "        out = self(token_ids, attention_mask, intent_target, slots_target)\n",
    "        intent_pred, slot_pred = out[\"intent_pred\"], out[\"slot_pred\"]\n",
    "\n",
    "        self.log(\n",
    "            \"val_IC_NER_loss\",\n",
    "            out[\"joint_loss\"],\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"val_IC_loss\", out[\"ic_loss\"], on_step=False, on_epoch=True, logger=True\n",
    "        )\n",
    "        self.log(\n",
    "            \"val_NER_loss\", out[\"ner_loss\"], on_step=False, on_epoch=True, logger=True\n",
    "        )\n",
    "        self.log(\n",
    "            \"val_intent_acc\",\n",
    "            accuracy(out[\"intent_pred\"], intent_target),\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"slot_f1\",\n",
    "            slot_F1(out[\"slot_pred\"], slots_target, idx2slots),\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            logger=True,\n",
    "        )\n",
    "\n",
    "        return out[\"joint_loss\"]\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "\n",
    "        token_ids, attention_mask = batch[\"token_ids\"], batch[\"mask\"]\n",
    "        intent_target, slots_target = batch[\"intent_id\"], batch[\"slots_id\"]\n",
    "\n",
    "        out = self(token_ids, attention_mask, intent_target, slots_target)\n",
    "        intent_pred, slot_pred = out[\"intent_pred\"], out[\"slot_pred\"]\n",
    "\n",
    "        self.log(\n",
    "            \"test_intent_acc\",\n",
    "            accuracy(intent_pred, intent_target),\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"test_slot_f1\",\n",
    "            slot_F1(slot_pred, slots_target, idx2slots),\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            logger=True,\n",
    "        )\n",
    "\n",
    "        return out[\"joint_loss\"]\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(\n",
    "            self.parameters(), lr=self.lr, weight_decay=self.cfg[\"tc\"][\"weight_decay\"]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "79e85668",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "\n",
    "    # We optimize the number of layers, hidden units in each layer and dropouts.\n",
    "    ihidden_size = trial.suggest_int(\"intent_hidden_size\", 32, 512, log=True)\n",
    "    idropout_1 = trial.suggest_float(\"idropout1\", 0.2, 0.5)\n",
    "    idropout_2 = trial.suggest_float(\"idropout2\", 0.2, 0.5)\n",
    "    sdropout = trial.suggest_float(\"sdropout\", 0.2, 0.5)\n",
    "    lr = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1000)\n",
    "\n",
    "    model = jointBert(config, idropout_1, idropout_2, sdropout, ihidden_size, lr)\n",
    "\n",
    "    dm = NLU_Dataset_pl(\n",
    "        config[\"dc\"][\"train_dir\"],\n",
    "        config[\"dc\"][\"val_dir\"],\n",
    "        config[\"dc\"][\"val_dir\"],\n",
    "        config[\"mc\"][\"tokenizer_name\"],\n",
    "        config[\"dc\"][\"max_len\"],\n",
    "        config[\"tc\"][\"batch_size\"],\n",
    "        config[\"tc\"][\"num_worker\"],\n",
    "    )\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        logger=True,\n",
    "        checkpoint_callback=False,\n",
    "        max_epochs=13,\n",
    "        precision=config[\"misc\"][\"precision\"],\n",
    "        gpus=config[\"misc\"][\"gpus\"],\n",
    "        callbacks=[PyTorchLightningPruningCallback(trial, monitor=\"val_IC_NER_loss\")],\n",
    "    )\n",
    "    hyperparameters = dict(\n",
    "        hidden=ihidden_size,\n",
    "        idropout1=idropout_1,\n",
    "        idropout_2=idropout_2,\n",
    "        sdropout=sdropout,\n",
    "    )\n",
    "    trainer.logger.log_hyperparams(hyperparameters)\n",
    "    trainer.fit(model, datamodule=dm)\n",
    "\n",
    "    return trainer.callback_metrics[\"val_IC_NER_loss\"].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0cecba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-04-24 16:30:54,109]\u001b[0m A new study created in memory with name: no-name-03497d8a-5928-4b14-ac4b-a62987c72375\u001b[0m\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Using native 16bit precision.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name   | Type   | Params\n",
      "----------------------------------\n",
      "0 | IC_NER | IC_NER | 134 M \n",
      "----------------------------------\n",
      "134 M     Trainable params\n",
      "0         Non-trainable params\n",
      "134 M     Total params\n",
      "539.583   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2ed5121c9974a908326f1872852a1b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pruner = optuna.pruners.MedianPruner()\n",
    "study = optuna.create_study(direction=\"minimize\", pruner=pruner)\n",
    "study.optimize(objective, n_trials=100, timeout=1000)\n",
    "\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5baf7dfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
