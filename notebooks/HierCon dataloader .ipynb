{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "619855c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from functools import partial\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0566e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/BG_Noise_Phrase.txt\") as f:\n",
    "    content = f.readlines()\n",
    "phrase = [x.strip() for x in content]\n",
    "\n",
    "# contrastive noise augmentation samples\n",
    "\n",
    "\n",
    "def mergelistsBG(ns, s, prob):\n",
    "\n",
    "    noisySample = deepcopy(ns)\n",
    "    sample = deepcopy(s)\n",
    "\n",
    "    bernaulliSample = [0] * int((1000) * prob) + [1] * int(1000 * (1 - prob))\n",
    "    random.shuffle(bernaulliSample)\n",
    "\n",
    "    final = []\n",
    "\n",
    "    while len(noisySample) > 0 and len(sample) > 0:\n",
    "\n",
    "        if random.sample(bernaulliSample, 1)[0] == 0:\n",
    "            final.append(noisySample.pop(0))\n",
    "        else:\n",
    "            final.append(sample.pop(0))\n",
    "\n",
    "    if len(noisySample) == 0:\n",
    "        final = final + sample\n",
    "    else:\n",
    "        final = final + noisySample\n",
    "\n",
    "    return s,final\n",
    "\n",
    "def mergelistsMC(text_packed, prob):\n",
    "\n",
    "    text = deepcopy(text_packed)\n",
    "\n",
    "    bernaulliSample = [0] * int((1000) * prob) + [1] * int(1000 * (1 - prob))\n",
    "    random.shuffle(bernaulliSample)\n",
    "\n",
    "    orig,aug  = [text_packed[0]],[text_packed[0]]\n",
    "    for idx,tokens in enumerate(text_packed[1:]):\n",
    "        \n",
    "        if random.sample(bernaulliSample, 1)[0] == 0:\n",
    "            orig.append([tokens[0],'2000'])\n",
    "        else:\n",
    "            orig.append(tokens)\n",
    "            aug.append(tokens)\n",
    "\n",
    "    return orig,aug\n",
    "\n",
    "def contrastiveSampleGenerator(sample, noise_type):\n",
    "\n",
    "    samplePacked = [[token, str(idx)] for idx, token in enumerate(sample.split())]\n",
    "\n",
    "    noisyTEXT = random.sample(phrase, 3)\n",
    "    noisyTEXT = (noisyTEXT[0] + noisyTEXT[1] + noisyTEXT[2]).split()\n",
    "    noisyTOKENS = random.sample(noisyTEXT, random.sample([5, 6, 7,8,9,10], 1)[0])\n",
    "    noisyPacked = [[token, '2000'] for idx, token in enumerate(noisyTOKENS)]\n",
    "\n",
    "    if noise_type == 'MC':\n",
    "        noise_param = random.sample([0.20,0.40,0.60],1)[0]\n",
    "        orig, aug = mergelistsMC(samplePacked, prob=noise_param)\n",
    "        augText, augSlots = zip(*aug)\n",
    "        origText, origSlots = zip(*orig)\n",
    "\n",
    "        return ' '.join(list(origText)), ' '.join(list(augText)), ' '.join(list(origSlots)), ' '.join(list(augSlots))\n",
    "\n",
    "    elif noise_type == 'BG':\n",
    "        noise_param = random.sample([0.20,0.40,0.60],1)[0]\n",
    "        orig, aug  = mergelistsBG(noisyPacked,samplePacked,  prob=noise_param)\n",
    "        augText, augSlots = zip(*aug)\n",
    "        origText, origSlots = zip(*orig)\n",
    "        return ' '.join(list(origText)), ' '.join(list(augText)), ' '.join(list(origSlots)), ' '.join(list(augSlots))\n",
    "\n",
    "def contrastivePairs(text, noise_type):\n",
    "\n",
    "    textP1, textP2, slotsID1, slotsID2, sentID1, sentID2 = [], [], [], [], [], []\n",
    "\n",
    "    for idx, sample in enumerate(text):\n",
    "\n",
    "        origText,augText, origSlots, augSlots = contrastiveSampleGenerator(sample,noise_type)\n",
    "          \n",
    "        textP1.append(origText)\n",
    "        slotsID1.append(origSlots)\n",
    "        sentID1.append(idx)\n",
    "\n",
    "        textP2.append(augText)\n",
    "        slotsID2.append(augSlots)\n",
    "        sentID2.append(idx)\n",
    "\n",
    "    return textP1, textP2, slotsID1, slotsID2, sentID1, sentID2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b16fd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_CT(batch, tokenizer, noise_type):\n",
    "\n",
    "    text,intent_id,slot_id = [],[],[]\n",
    "    \n",
    "    for datapoint in batch:\n",
    "        text.append(datapoint['text'])\n",
    "        intent_id.append(datapoint['intent_id'])\n",
    "        slot_id.append(datapoint['slots_id'])\n",
    "\n",
    "    # processing batch for supervised learning\n",
    "    # tokenization and packing to torch tensor\n",
    "    token_ids, mask, slots_ids = batch_tokenizer(text, slot_id, tokenizer)\n",
    "    token_ids, mask, intent_id, slots_ids = list2Tensor(\n",
    "        [token_ids, mask, intent_id, slots_ids]\n",
    "    )\n",
    "\n",
    "    supBatch = {\n",
    "        \"token_ids\": token_ids,\n",
    "        \"mask\": mask,\n",
    "        \"intent_id\": intent_id,\n",
    "        \"slots_id\": slots_ids,\n",
    "    }\n",
    "\n",
    "    # processing batch for hierarchial contrastive learning\n",
    "\n",
    "    # generating contrastive pairs\n",
    "    textP1, textP2, tokenID1, tokenID2, sentID1, sentID2 = contrastivePairs(\n",
    "        text,noise_type\n",
    "    )\n",
    "\n",
    "    # tokenization and packing for pair 1\n",
    "    token_ids1, mask1, processed_tokenID1 = batch_tokenizer(textP1, tokenID1, tokenizer)\n",
    "    token_ids1, mask1, sentID1, packed_tokenID1 = list2Tensor(\n",
    "        [token_ids1, mask1, sentID1, processed_tokenID1]\n",
    "    )\n",
    "\n",
    "    # tokenization and packing for pair 2\n",
    "    token_ids2, mask2, processed_tokenID2 = batch_tokenizer(textP2, tokenID2, tokenizer)\n",
    "    token_ids2, mask2, sentID2, packed_tokenID2 = list2Tensor(\n",
    "        [token_ids2, mask2, sentID2, processed_tokenID2]\n",
    "    )\n",
    "\n",
    "    CP1 = {\n",
    "        \"token_ids\": token_ids1,\n",
    "        \"mask\": mask1,\n",
    "        \"sent_id\": sentID1,\n",
    "        \"token_id\": packed_tokenID1,\n",
    "    }\n",
    "\n",
    "    CP2 = {\n",
    "        \"token_ids\": token_ids2,\n",
    "        \"mask\": mask2,\n",
    "        \"sent_id\": sentID2,\n",
    "        \"token_id\": packed_tokenID2,\n",
    "    }\n",
    "\n",
    "    return {\"supBatch\": supBatch, \"HCLBatch\": [CP1, CP2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74d29e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "    def __init__(self, file_dir):\n",
    "\n",
    "        self.data = pd.read_csv(file_dir, sep=\"\\t\")\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        # text\n",
    "        text = str(self.data.TEXT[index])\n",
    "        \n",
    "        # intent\n",
    "        intent_label = self.data.INTENT[index]\n",
    "        intent_id = self.data.INTENT_ID[index]\n",
    "        \n",
    "        # slots\n",
    "        slot_label = self.data.SLOTS[index]\n",
    "        slot_id = self.data.SLOTS_ID[index]\n",
    "\n",
    "        return {\n",
    "\n",
    "            \"text\": text,\n",
    "\n",
    "            \"intent_id\": intent_id,\n",
    "            \"intent_label\": intent_label,\n",
    "\n",
    "            \"slots_id\": slot_id,\n",
    "            \"slots_label\": slot_label,\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0e6bac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataloader(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__( self,train_dir,batch_size,num_workers):\n",
    "\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.num_worker = num_workers\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('distilbert-base-cased',cache_dir = '/efs-storage/tokenizer/')\n",
    "        self.mode = args.mode\n",
    "        self.args = args\n",
    "\n",
    "    def setup(self, stage: [str] = None):\n",
    "\n",
    "        self.train = dataset(self.train_dir)\n",
    "\n",
    "        self.val = dataset(self.val_dir)\n",
    "\n",
    "        if self.mode == 'BASELINE':\n",
    "            self.train_collate = partial(collate_sup,tokenizer = self.tokenizer)\n",
    "            self.val_collate = partial(collate_sup, tokenizer = self.tokenizer)\n",
    "        \n",
    "        elif self.mode == 'AT':\n",
    "            self.train_collate = partial(collate_AT,tokenizer = self.tokenizer, noise_type = self.args.noise_type)\n",
    "            self.val_collate = partial(collate_sup,tokenizer = self.tokenizer)\n",
    "        \n",
    "        elif self.mode == 'CT':\n",
    "            self.train_collate = partial(collate_CT,tokenizer = self.tokenizer, noise_type = self.args.noise_type)\n",
    "            self.val_collate = partial(collate_sup,tokenizer = self.tokenizer)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train, batch_size=self.batch_size, shuffle=True, collate_fn=self.train_collate, num_workers=self.num_worker\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val, batch_size=self.batch_size, collate_fn=self.val_collate, num_workers=self.num_worker\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8170bcb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277524c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
