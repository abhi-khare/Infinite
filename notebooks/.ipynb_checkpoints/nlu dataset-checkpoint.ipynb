{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from transformers import DistilBertTokenizerFast\n",
    "import pandas as pd\n",
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nluDataset(Dataset):\n",
    "\n",
    "    def __init__(self, file_dir, tokenizer, max_len, device):\n",
    "        \n",
    "        self.data = pd.read_csv(file_dir, sep='\\t')\n",
    "        self.tokenizer = DistilBertTokenizerFast.from_pretrained(tokenizer)\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def processSlotLabel(self,word_ids,slot_ids,text):\n",
    "        \n",
    "        slot_ids = list(map(int, slot_ids.split(' ')))\n",
    "        try:\n",
    "            new_labels = [-100 if idx==None else slot_ids[idx] for idx in word_ids]\n",
    "            previous_word_idx = -100\n",
    "        #print(word_ids,slot_ids)\n",
    "            for idx,_ in enumerate(new_labels[1:]):\n",
    "            \n",
    "                if _ == -100:\n",
    "                    continue\n",
    "                if _ == previous_word_idx:\n",
    "                    new_labels[idx+1] = -100\n",
    "            \n",
    "            \n",
    "                previous_word_idx = _\n",
    "                \n",
    "            return new_labels\n",
    "        except:\n",
    "            print(slot_ids,word_ids,text)\n",
    "        \n",
    "        return [1]*56\n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        text = str(self.data.TEXT[index])\n",
    "        text = text.replace('.','')\n",
    "        text = text.replace('\\'','')\n",
    "        text = \" \".join(text.split())\n",
    "        \n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            return_token_type_ids=False,\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            \n",
    "            #is_split_into_words=True\n",
    "        )\n",
    "        \n",
    "       # print(self.tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"]),inputs.word_ids())\n",
    "        # text encoding\n",
    "        token_ids = torch.tensor(inputs['input_ids'], dtype=torch.long)\n",
    "        mask = torch.tensor(inputs['attention_mask'], dtype=torch.long)\n",
    "        word_ids = inputs.word_ids()\n",
    "\n",
    "        # intent\n",
    "        intent_id = torch.tensor(self.data.INTENT_ID[index], dtype=torch.long)\n",
    "        intent_label = self.data.INTENT[index]\n",
    "\n",
    "        # label processing\n",
    "        slot_label = self.data.SLOTS[index]\n",
    "        slot_id = self.processSlotLabel(word_ids,self.data.SLOTS_ID[index],text)\n",
    "    \n",
    "        slot_id = torch.tensor(slot_id,dtype=torch.long)\n",
    "        \n",
    "\n",
    "        #language = self.data.language[index]\n",
    "        \n",
    "        return {\n",
    "            'token_ids': token_ids,\n",
    "            'mask': mask,\n",
    "            'intent_id': intent_id,\n",
    "            'slots_id' : slot_id,\n",
    "            'intent_label': intent_label,\n",
    "            'slots_label' : slot_label\n",
    "        } \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = nluDataset(file_dir='../data/multiATIS/split/test/OOC_test/0_25n/test_EN.tsv', tokenizer='distilbert-base-multilingual-cased', max_len=56, device=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(ds,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dl:\n",
    "    a= 1 #print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
