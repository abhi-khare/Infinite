{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffeee68f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 53044,
     "status": "ok",
     "timestamp": 1624439223540,
     "user": {
      "displayName": "abhinash khare",
      "photoUrl": "",
      "userId": "15702289059838736630"
     },
     "user_tz": -330
    },
    "id": "ffeee68f",
    "outputId": "aa9f1dff-f1ae-40c1-b749-ef428081fadf"
   },
   "outputs": [],
   "source": [
    "# uncomment below lines if you are running it in google colab\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "#import sys\n",
    "#sys.path.append('/content/drive/MyDrive/research/Infinite/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f719273",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install optuna\n",
    "! pip install pytorch-lightning\n",
    "! pip install transformers\n",
    "! pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e140ffd9",
   "metadata": {
    "id": "e140ffd9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import DistilBertModel, DistilBertTokenizerFast\n",
    "import pytorch_lightning as pl\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from seqeval.metrics import f1_score\n",
    "from seqeval.metrics import accuracy_score\n",
    "from seqeval.scheme import IOB2\n",
    "\n",
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b8e1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basePath = '/content/drive/MyDrive/research/Infinite/' # comment this if running on colab\n",
    "basePath = './' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077a9145",
   "metadata": {
    "id": "077a9145"
   },
   "outputs": [],
   "source": [
    "# loading slot index file\n",
    "final_slots = pd.read_csv( basePath + \"data/SNIPS/slot_list.tsv\", sep=\",\", header=None, names=[\"SLOTS\"]\n",
    ").SLOTS.values.tolist()\n",
    "\n",
    "idx2slots = {idx: slots for idx, slots in enumerate(final_slots)}\n",
    "\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2b490a",
   "metadata": {
    "id": "4d2b490a"
   },
   "outputs": [],
   "source": [
    "# model parameter\n",
    "config = {\n",
    "    \"mc\": {\n",
    "        \"model_name\": \"distilbert-base-cased\",\n",
    "        \"tokenizer_name\": \"distilbert-base-cased\",\n",
    "        \"joint_loss_coef\": 0.5,\n",
    "    },\n",
    "    # training parameters\n",
    "    \"tc\": {\n",
    "        \"encoder_lr\": 0.00002,\n",
    "        \"epoch\": 15,\n",
    "        \"batch_size\": 64,\n",
    "        \"weight_decay\": 0.003,\n",
    "        \"shuffle_data\": True,\n",
    "        \"num_worker\": 2\n",
    "    },\n",
    "    # data params\n",
    "    \"dc\": {\n",
    "        \"train_dir\": basePath +  \"data/SNIPS/experiments/train/clean/train.tsv\",\n",
    "        \"val_dir\": basePath + \"data/SNIPS/experiments/dev/clean/dev.tsv\",\n",
    "        \"max_len\": 56,\n",
    "    },\n",
    "    # misc\n",
    "    \"misc\": {\n",
    "        \"fix_seed\": False,\n",
    "        \"gpus\": -1,\n",
    "        \"precision\": 16,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IigQNMhD7pus",
   "metadata": {
    "id": "IigQNMhD7pus"
   },
   "outputs": [],
   "source": [
    "def slot_F1(pred,target,id2slots):\n",
    "\n",
    "    pred_list = pred.tolist()\n",
    "    target_list = target.tolist()\n",
    "    \n",
    "    pred_slots , target_slots = [],[]\n",
    "\n",
    "    for idx,sample in enumerate(target_list):\n",
    "        pred_sample,target_sample = [],[]\n",
    "        for jdx,slot in enumerate(sample):\n",
    "\n",
    "            if (slot == -100 or slot==0)!=True:\n",
    "                target_sample.append(id2slots[slot])\n",
    "                pred_sample.append(id2slots[pred_list[idx][jdx]])\n",
    "\n",
    "        pred_slots.append(pred_sample)\n",
    "        target_slots.append(target_sample)\n",
    "    \n",
    "    return f1_score( target_slots, pred_slots,mode='strict', scheme=IOB2)\n",
    "\n",
    "\n",
    "def accuracy(pred,target):\n",
    "    return torch.sum(pred==target)/len(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0PSZKJTX7XQA",
   "metadata": {
    "id": "0PSZKJTX7XQA"
   },
   "outputs": [],
   "source": [
    "class nluDataset(Dataset):\n",
    "    def __init__(self, file_dir, tokenizer, max_len):\n",
    "\n",
    "        self.data = pd.read_csv(file_dir, sep=\"\\t\")\n",
    "        self.tokenizer = DistilBertTokenizerFast.from_pretrained(tokenizer)\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def processSlotLabel(self, word_ids, slot_ids):\n",
    "\n",
    "        # replace None and repetition with -100\n",
    "        word_ids = [-100 if word_id == None else word_id for word_id in word_ids]\n",
    "        previous_word = -100\n",
    "        for idx, wid in enumerate(word_ids):\n",
    "\n",
    "            if wid == -100:\n",
    "                continue\n",
    "            if wid == previous_word:\n",
    "                word_ids[idx] = -100\n",
    "            previous_word = wid\n",
    "\n",
    "        Pslot_ids = []\n",
    "        for sid in slot_ids.split():\n",
    "            Pslot_ids.append(int(sid))\n",
    "        \n",
    "        new_labels = [-100 if word_id == -100 else Pslot_ids[word_id] for word_id in word_ids\n",
    "        return new_labels\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        text = str(self.data.TEXT[index])\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text.split(),\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            return_token_type_ids=False,\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            is_split_into_words=True\n",
    "        )\n",
    "\n",
    "        # text encoding\n",
    "        token_ids = torch.tensor(inputs[\"input_ids\"], dtype=torch.long)\n",
    "        mask = torch.tensor(inputs[\"attention_mask\"], dtype=torch.long)\n",
    "        word_ids = inputs.word_ids()\n",
    "\n",
    "        # intent\n",
    "        intent_id = torch.tensor(self.data.INTENT_ID[index], dtype=torch.long)\n",
    "        intent_label = self.data.INTENT[index]\n",
    "\n",
    "        # label processing\n",
    "        slot_label = self.data.SLOTS[index]\n",
    "        slot_id = self.processSlotLabel(word_ids, self.data.SLOTS_ID[index])\n",
    "\n",
    "        slot_id = torch.tensor(slot_id, dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            \"token_ids\": token_ids,\n",
    "            \"mask\": mask,\n",
    "            \"intent_id\": intent_id,\n",
    "            \"slots_id\": slot_id,\n",
    "            \"intent_label\": intent_label,\n",
    "            \"slots_label\": slot_label,\n",
    "            \"text\": text,\n",
    "            \"slotsID\": self.data.SLOTS_ID[index],\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "class NLU_Dataset_pl(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self, train_dir, val_dir, test_dir, tokenizer, max_len, batch_size, num_worker\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.train_dir = train_dir\n",
    "        self.val_dir = val_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.num_worker = num_worker\n",
    "\n",
    "    def setup(self, stage: [str] = None):\n",
    "        self.train = nluDataset(self.train_dir, self.tokenizer, self.max_len)\n",
    "\n",
    "        self.val = nluDataset(self.val_dir, self.tokenizer, self.max_len)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train, batch_size=self.batch_size, shuffle=True, num_workers=self.num_worker\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val, batch_size=self.batch_size, num_workers=self.num_worker\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19966fc",
   "metadata": {
    "id": "a19966fc"
   },
   "outputs": [],
   "source": [
    "class IC_NER(nn.Module):\n",
    "    def __init__(self, idropout_1, idropout_2, sdropout, ihidden_size):\n",
    "\n",
    "        super(IC_NER, self).__init__()\n",
    "\n",
    "        self.encoder = DistilBertModel.from_pretrained(\n",
    "            config['mc']['model_name'],\n",
    "            return_dict=True,\n",
    "            output_hidden_states=True,\n",
    "            sinusoidal_pos_embds=True\n",
    "        )\n",
    "\n",
    "        self.intent_dropout_1 = nn.Dropout(idropout_1)\n",
    "        self.intent_dropout_2 = nn.Dropout(idropout_2)\n",
    "        self.intent_FC1 = nn.Linear(768, ihidden_size)\n",
    "        self.intent_FC2 = nn.Linear(ihidden_size, 8)\n",
    "\n",
    "        # slots layer\n",
    "        self.slots_dropout = nn.Dropout(sdropout)\n",
    "        self.slots_FC = nn.Linear(768, 72)\n",
    "\n",
    "        self.intent_loss_fn = nn.CrossEntropyLoss()\n",
    "        self.slot_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.jlc = 0.5\n",
    "        # self.cfg = cfg\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, intent_target, slots_target):\n",
    "\n",
    "        encoded_output = self.encoder(input_ids, attention_mask)\n",
    "\n",
    "        # intent data flow\n",
    "        intent_hidden = encoded_output[0][:, 0]\n",
    "        intent_hidden = self.intent_FC1(self.intent_dropout_1(F.gelu(intent_hidden)))\n",
    "        intent_logits = self.intent_FC2(self.intent_dropout_2(F.gelu(intent_hidden)))\n",
    "\n",
    "        # accumulating intent classification loss\n",
    "        intent_loss = self.intent_loss_fn(intent_logits, intent_target)\n",
    "        intent_pred = torch.argmax(nn.Softmax(dim=1)(intent_logits), axis=1)\n",
    "\n",
    "        # slots data flow\n",
    "        slots_hidden = encoded_output[0]\n",
    "        slots_logits = self.slots_FC(self.slots_dropout(F.relu(slots_hidden)))\n",
    "        slot_pred = torch.argmax(nn.Softmax(dim=2)(slots_logits), axis=2)\n",
    "\n",
    "        # accumulating slot prediction loss\n",
    "        slot_loss = self.slot_loss_fn(slots_logits.view(-1, 72), slots_target.view(-1))\n",
    "\n",
    "        joint_loss = self.jlc * intent_loss + (1.0 - self.jlc) * slot_loss\n",
    "\n",
    "        return {\n",
    "            \"joint_loss\": joint_loss,\n",
    "            \"ic_loss\": intent_loss,\n",
    "            \"ner_loss\": slot_loss,\n",
    "            \"intent_pred\": intent_pred,\n",
    "            \"slot_pred\": slot_pred,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e85668",
   "metadata": {
    "id": "79e85668"
   },
   "outputs": [],
   "source": [
    "trial_cnt = 0\n",
    "\n",
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "\n",
    "    # We optimize the number of layers, hidden units in each layer and dropouts.\n",
    "    ihidden_size = trial.suggest_int(\"intent_hidden_size\", 64, 512)\n",
    "    \n",
    "    idropout_1 = trial.suggest_float(\"idropout1\", 0.2, 0.5)\n",
    "    idropout_2 = trial.suggest_float(\"idropout2\", 0.2, 0.5)\n",
    "    sdropout = trial.suggest_float(\"sdropout\", 0.2, 0.5)\n",
    "\n",
    "    intentLR = trial.suggest_float(\"intentLR\", 0.00001, 0.001)\n",
    "    slotsLR = trial.suggest_float(\"slotsLR\", 0.00001, 0.001)\n",
    "\n",
    "\n",
    "    \n",
    "    model = IC_NER(idropout_1, idropout_2, sdropout, ihidden_size).to(DEVICE)\n",
    "\n",
    "    dm = NLU_Dataset_pl(\n",
    "        config[\"dc\"][\"train_dir\"],\n",
    "        config[\"dc\"][\"val_dir\"],\n",
    "        config[\"mc\"][\"tokenizer_name\"],\n",
    "        config[\"dc\"][\"max_len\"],\n",
    "        config[\"tc\"][\"batch_size\"],\n",
    "        config[\"tc\"][\"num_worker\"],\n",
    "    )\n",
    "    dm.setup()\n",
    "    \n",
    "    trainDL, valDL = dm.train_dataloader() , dm.val_dataloader()\n",
    "    \n",
    "    optimizer = torch.optim.AdamW([\n",
    "                {'params': model.encoder.parameters() , 'lr' : 0.00005 , 'weight_decay': config[\"tc\"][\"weight_decay\"]},\n",
    "                {'params': model.intent_FC1.parameters(), 'lr': intentLR},\n",
    "                {'params': model.intent_FC2.parameters(), 'lr': intentLR},\n",
    "                {'params': model.slots_FC.parameters(), 'lr': slotsLR}])\n",
    "\n",
    "    # training\n",
    "    # training\n",
    "    model.train()\n",
    "    for epoch in range(config['tc']['epoch']):\n",
    "        \n",
    "        for batch in trainDL:\n",
    "            token_ids, attention_mask = batch[\"token_ids\"].to(DEVICE), batch[\"mask\"].to(DEVICE)\n",
    "            intent_target, slots_target = batch[\"intent_id\"].to(DEVICE), batch[\"slots_id\"].to(DEVICE)\n",
    "\n",
    "            out = model(token_ids, attention_mask, intent_target, slots_target)\n",
    "            optimizer.zero_grad()\n",
    "            out[\"joint_loss\"].backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    #validation\n",
    "\n",
    "    acc,slotsF1,cnt = 0.0,0.0,0\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for batch in valDL:\n",
    "\n",
    "            token_ids, attention_mask = batch[\"token_ids\"].to(DEVICE), batch[\"mask\"].to(DEVICE)\n",
    "            intent_target, slots_target = batch[\"intent_id\"].to(DEVICE), batch[\"slots_id\"].to(DEVICE)\n",
    "\n",
    "            out = model(token_ids, attention_mask, intent_target, slots_target)\n",
    "            intent_pred, slot_pred = out[\"intent_pred\"], out[\"slot_pred\"]\n",
    "            \n",
    "            acc += accuracy(out[\"intent_pred\"], intent_target)\n",
    "            slotsF1 += slot_F1(out[\"slot_pred\"], slots_target, idx2slots)\n",
    "            cnt += 1\n",
    "        \n",
    "    acc = acc/float(cnt)\n",
    "    slotsF1 = slotsF1/float(cnt)\n",
    "\n",
    "    return acc, slotsF1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0cecba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ea0cecba",
    "outputId": "0b6e78d0-b21d-4e3e-bc67-f4b3be1ac32d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: ExperimentalWarning:\n",
      "\n",
      "MOTPESampler is experimental (supported from v2.4.0). The interface can change in the future.\n",
      "\n",
      "\u001b[32m[I 2021-06-23 09:42:03,990]\u001b[0m A new study created in memory with name: no-name-1b72d1ac-2686-4575-a286-5b86b3166f1b\u001b[0m\n",
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\u001b[32m[I 2021-06-23 09:51:13,215]\u001b[0m Trial 0 finished with values: [0.9840909242630005, 0.9513162328008591] and parameters: {'intent_hidden_size': 223, 'idropout1': 0.23914783753136815, 'idropout2': 0.3453831608644609, 'sdropout': 0.3866062743395723, 'intentLR': 0.00022741524086434224, 'slotsLR': 0.00014328190096762704}. \u001b[0m\n",
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\u001b[32m[I 2021-06-23 10:00:23,612]\u001b[0m Trial 1 finished with values: [0.9840909242630005, 0.9545181762559177] and parameters: {'intent_hidden_size': 69, 'idropout1': 0.4832580259647837, 'idropout2': 0.26104362711860485, 'sdropout': 0.29948132051449505, 'intentLR': 0.0005858002605686029, 'slotsLR': 0.0008850252733270189}. \u001b[0m\n",
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\u001b[32m[I 2021-06-23 10:09:35,710]\u001b[0m Trial 2 finished with values: [0.9883522987365723, 0.9513790786076037] and parameters: {'intent_hidden_size': 459, 'idropout1': 0.3120135998468671, 'idropout2': 0.3942931901430078, 'sdropout': 0.33999764661006265, 'intentLR': 0.0004650871210549611, 'slotsLR': 0.0007196099980619471}. \u001b[0m\n",
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\u001b[32m[I 2021-06-23 10:18:48,276]\u001b[0m Trial 3 finished with values: [0.9898675084114075, 0.9558596892329845] and parameters: {'intent_hidden_size': 440, 'idropout1': 0.23740369943465148, 'idropout2': 0.36886032202083086, 'sdropout': 0.23017491814054658, 'intentLR': 0.0003780459727740985, 'slotsLR': 0.0001748742430446569}. \u001b[0m\n",
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\u001b[32m[I 2021-06-23 10:28:01,317]\u001b[0m Trial 4 finished with values: [0.9869318008422852, 0.9548496722255119] and parameters: {'intent_hidden_size': 229, 'idropout1': 0.39304016876285636, 'idropout2': 0.35032362827454167, 'sdropout': 0.22734149892653727, 'intentLR': 0.00044702481780861555, 'slotsLR': 0.0008544454259355506}. \u001b[0m\n",
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\u001b[32m[I 2021-06-23 10:37:14,275]\u001b[0m Trial 5 finished with values: [0.981249988079071, 0.9462622155863962] and parameters: {'intent_hidden_size': 245, 'idropout1': 0.42126665998560775, 'idropout2': 0.4705312138358925, 'sdropout': 0.2016193251589378, 'intentLR': 0.0006750857836062053, 'slotsLR': 0.00022897987506842563}. \u001b[0m\n",
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "sampler = optuna.samplers.MOTPESampler(n_startup_trials=21)\n",
    "study = optuna.create_study(directions=[\"maximize\",\"maximize\"])\n",
    "study.optimize(objective, n_trials=30, timeout=100000)\n",
    "\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5baf7dfa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 511,
     "status": "ok",
     "timestamp": 1623248675357,
     "user": {
      "displayName": "abhinash khare",
      "photoUrl": "",
      "userId": "15702289059838736630"
     },
     "user_tz": -330
    },
    "id": "5baf7dfa",
    "outputId": "4d49afc0-8a82-402c-e345-aa492dd64212"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials: [FrozenTrial(number=0, values=None, datetime_start=datetime.datetime(2021, 6, 9, 14, 24, 22, 305204), datetime_complete=None, params={'intent_hidden_size': 399, 'idropout1': 0.260516762307083, 'idropout2': 0.22715500475659603, 'sdropout': 0.31020657299487797}, distributions={'intent_hidden_size': IntUniformDistribution(high=512, low=64, step=1), 'idropout1': UniformDistribution(high=0.5, low=0.2), 'idropout2': UniformDistribution(high=0.5, low=0.2), 'sdropout': UniformDistribution(high=0.5, low=0.2)}, user_attrs={}, system_attrs={'nsga2:generation': 0}, intermediate_values={}, trial_id=0, state=TrialState.RUNNING, value=None)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of finished trials: {}\".format((study.trials)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3oMRGPsJjUcn",
   "metadata": {
    "id": "3oMRGPsJjUcn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "jointBert_Tuning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
